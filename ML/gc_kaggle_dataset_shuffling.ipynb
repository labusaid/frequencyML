{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lJWBHe0ZgE6e"
   },
   "outputs": [],
   "source": [
    "#Block used to upload kaggle.json, for access to Kaggle API\n",
    "from google.colab import files\n",
    "uploaded = files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GCWe-Ofxnvc3"
   },
   "outputs": [],
   "source": [
    "#block for installing kaggle, creation of kaggle dir, and copying the json file over to current dir\n",
    "! pip install kaggle\n",
    "! mkdir ~/.kaggle\n",
    "! cp kaggle.json ~/.kaggle/\n",
    "! chmod 600 ~/.kaggle/kaggle.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g76Mh8Uqn4AQ"
   },
   "outputs": [],
   "source": [
    "#Block for downloading dataset\n",
    "! kaggle datasets download pinxau1000/radioml2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VeMhGhxppIrd"
   },
   "outputs": [],
   "source": [
    "#Block for unziping downloaded file\n",
    "! unzip radioml2018.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "fY1VFzR9ropQ"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-70bfe02b61f3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Open the dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "#code block for loading in DeepSig data from hdf5 file\n",
    "import tensorflow as tf\n",
    "import h5py\n",
    "import numpy as np\n",
    "import json\n",
    "import pandas as pd\n",
    "import math\n",
    "import sklearn\n",
    "\n",
    "# Open the dataset\n",
    "hdf5_file = h5py.File(\"../dataset/2018.01/GOLD_XYZ_OSC.0001_1024.hdf5\",  'r')\n",
    "# Load the modulation classes. You can also copy and paste the content of classes-fixed.txt.\n",
    "modulation_classes = json.load(open(\"../dataset/2018.01/classes-fixed.json\", 'r'))\n",
    "#List groups of the hdf5 file\n",
    "list(hdf5_file.keys())\n",
    "\n",
    "#opening up new hdf5 file\n",
    "f_shuffle = h5py.File(\"../dataset/2018.01/DeepSig_XYZ_OSC.0001_1024_Shuffled.hdf5\", \"a\")\n",
    "#creating the file groups\n",
    "x_shuf = f_shuffle.create_dataset(\"X\", (2555904,1024,2), dtype='f')\n",
    "y_shuf = f_shuffle.create_dataset(\"Y\", (2555904, 24), dtype='i')\n",
    "z_shuf = f_shuffle.create_dataset(\"Z\", (2555904,1), dtype='i')\n",
    "#creating the datasets in each group\n",
    "\n",
    "#indexes representing the quarters of the dataset\n",
    "#used to take a quarter of the original database at a time, shuffling it, and storing it into new hdf5\n",
    "qrt_one = 638976\n",
    "qrt_two = 1277952\n",
    "qrt_three = 1916928\n",
    "\n",
    "\n",
    "# Read the HDF5 groups, loading the entire group into numpy arrays\n",
    "#samples = hdf5_file['X'][:638976:]\n",
    "#modulation_onehot = hdf5_file['Y'][:638976:]\n",
    "#snr = hdf5_file['Z'][:638976:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "wpoSHsQarpcx"
   },
   "outputs": [],
   "source": [
    "# Read the HDF5 groups, loading a quarter of the dataset object into numpy arrays\n",
    "samples = hdf5_file['X'][:qrt_one:]\n",
    "modulation_onehot = hdf5_file['Y'][:qrt_one:]\n",
    "snr = hdf5_file['Z'][:qrt_one:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "pV1Ncegvrp3V"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-2743bf0d8bf2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#creation of  shuffled arrays from DeepSig dataset using sklearns's shuffle function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0msamples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodulation_onehot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msnr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodulation_onehot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msnr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#debugging print statements\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "#creation of  shuffled arrays from DeepSig dataset using sklearns's shuffle function\n",
    "from sklearn.utils import shuffle\n",
    "samples, modulation_onehot, snr = shuffle(samples, modulation_onehot, snr, random_state=0)\n",
    "\n",
    "#debugging print statements\n",
    "print(samples)\n",
    "print(modulation_onehot)\n",
    "print(snr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "PbeksqjrrqD7"
   },
   "outputs": [],
   "source": [
    "#writing to the new hdf5 dataset objects\n",
    "x_shuf[:qrt_one:] = samples[::]\n",
    "y_shuf[:qrt_one:] = modulation_onehot[::]\n",
    "z_shuf[:qrt_one:] = snr[::]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BjwOXs7ZA5oV"
   },
   "outputs": [],
   "source": [
    "#repeat the process for each quarter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "jmJ_ylcWrqKA"
   },
   "outputs": [],
   "source": [
    "# Read the HDF5 groups, loading a quarter of the dataset object into numpy arrays\n",
    "samples = hdf5_file['X'][qrt_one:qrt_two:]\n",
    "modulation_onehot = hdf5_file['Y'][qrt_one:qrt_two:]\n",
    "snr = hdf5_file['Z'][qrt_one:qrt_two:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8pyV-uWotDLc"
   },
   "outputs": [],
   "source": [
    "#creation of  shuffled arrays from DeepSig dataset using sklearns's shuffle function\n",
    "from sklearn.utils import shuffle\n",
    "samples, modulation_onehot, snr = shuffle(samples, modulation_onehot, snr, random_state=0)\n",
    "\n",
    "print(samples)\n",
    "print(modulation_onehot)\n",
    "print(snr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "1i2HWDNrtDSI"
   },
   "outputs": [],
   "source": [
    "#writing to the new hdf5 dataset objects\n",
    "x_shuf[qrt_one:qrt_two:] = samples[::]\n",
    "y_shuf[qrt_one:qrt_two:] = modulation_onehot[::]\n",
    "z_shuf[qrt_one:qrt_two:] = snr[::]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "5KKUzfmUt1m6"
   },
   "outputs": [],
   "source": [
    "# Read the HDF5 groups, loading a quarter of the dataset object into numpy arrays\n",
    "samples = hdf5_file['X'][qrt_two:qrt_three:]\n",
    "modulation_onehot = hdf5_file['Y'][qrt_two:qrt_three:]\n",
    "snr = hdf5_file['Z'][qrt_two:qrt_three:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8tN13Enzt1uQ"
   },
   "outputs": [],
   "source": [
    "#creation of  shuffled arrays from DeepSig dataset using sklearns's shuffle function\n",
    "from sklearn.utils import shuffle\n",
    "samples, modulation_onehot, snr = shuffle(samples, modulation_onehot, snr, random_state=0)\n",
    "\n",
    "print(samples)\n",
    "print(modulation_onehot)\n",
    "print(snr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "LtHCuTqxt1ye"
   },
   "outputs": [],
   "source": [
    "#writing to the new hdf5 dataset objects\n",
    "x_shuf[qrt_two:qrt_three:] = samples[::]\n",
    "y_shuf[qrt_two:qrt_three:] = modulation_onehot[::]\n",
    "z_shuf[qrt_two:qrt_three:] = snr[::]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "QU_q7RNEu7w8"
   },
   "outputs": [],
   "source": [
    "# Read the HDF5 groups, loading a quarter of the dataset object into numpy arrays\n",
    "samples = hdf5_file['X'][qrt_three::]\n",
    "modulation_onehot = hdf5_file['Y'][qrt_three::]\n",
    "snr = hdf5_file['Z'][qrt_three::]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qCNqMr19u718"
   },
   "outputs": [],
   "source": [
    "#creation of  shuffled arrays from DeepSig dataset using sklearns's shuffle function\n",
    "from sklearn.utils import shuffle\n",
    "samples, modulation_onehot, snr = shuffle(samples, modulation_onehot, snr, random_state=0)\n",
    "\n",
    "print(samples)\n",
    "print(modulation_onehot)\n",
    "print(snr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "maX0mPkIu75D"
   },
   "outputs": [],
   "source": [
    "#writing to the new hdf5 dataset objects\n",
    "x_shuf[qrt_three::] = samples[::]\n",
    "y_shuf[qrt_three::] = modulation_onehot[::]\n",
    "z_shuf[qrt_three::] = snr[::]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "J74688_7vu8s"
   },
   "outputs": [],
   "source": [
    "#closing both files\n",
    "hdf5_file.close()\n",
    "f_shuffle.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j-fBElaAwexL",
    "outputId": "54ac4c80-05bb-44a2-8d77-93f5ac00374b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  adding: content/DeepSig_shuffled/ (stored 0%)\n",
      "  adding: content/DeepSig_shuffled/datasets.desktop (deflated 19%)\n",
      "  adding: content/DeepSig_shuffled/DeepSig_XYZ_OSC.0001_1024_Shuffled.hdf5 (deflated 8%)\n",
      "  adding: content/DeepSig_shuffled/classes-fixed.json (deflated 54%)\n",
      "  adding: content/DeepSig_shuffled/LICENSE.TXT (deflated 69%)\n",
      "  adding: content/DeepSig_shuffled/classes.txt (deflated 55%)\n",
      "  adding: content/DeepSig_shuffled/classes-fixed.txt (deflated 76%)\n"
     ]
    }
   ],
   "source": [
    "#manually moved new hdf5 file and other dataset description files(license, classes, etc.)\n",
    "#into one folder and zip it\n",
    "!zip -r /content/DS_shuffled.zip /content/DeepSig_shuffled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "R_8h3Ecx0Mp0"
   },
   "outputs": [],
   "source": [
    "#mounted google drive\n",
    "#move the zipped dataset file to google drive\n",
    "! mv ./DS_shuffled.zip ./drive/MyDrive "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "XpLwSaEJ4HcI"
   },
   "outputs": [],
   "source": [
    "#flush buffer and unmount google drive\n",
    "from google.colab import drive\n",
    "drive.flush_and_unmount()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "gc_kaggle_dataset_shuffling.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
